{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe9oBlcfeQ2a7ppM9UqnGP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanadoolsae/ewhaguidebook/blob/main/%08data/embedding/embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install konlpy\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "hwlmDmtOPPx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "import pickle\n",
        "import numpy as np  # Add this line\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "0tLLBB5GPWWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZl-iFX4XkJF"
      },
      "outputs": [],
      "source": [
        "# 파일에서 불용어 불러오기\n",
        "with open('/content/stopwords.txt', 'r', encoding='utf-8') as f:\n",
        "    file_stopwords = f.readlines()\n",
        "file_stopwords = [x.strip() for x in file_stopwords]\n",
        "\n",
        "# 불용어 리스트\n",
        "custom_stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '것', '라고', '에게', '라면', '을', '이라',\n",
        "             '라니', '있다', '아', '랑', '쯤된', '에서', '에선', '어', '이지만', '으로나', '때', '때는', '때라면', '때라서', '라', '이다', '있', '죠', '고', '니', '로', '있',\n",
        "             '같', '어서', '어요', '는데', '습니다', '면서', '많이', '마', '더', '그렇다', '의', '당', '좀', '책', '안', '볼', '게', '안', '정말', '듯', '이제야', '여', '요',\n",
        "             '게다가', '같다', '임', '로서', '이제', '만', '인', '붙이', '그', '저', '수', '가제', '부터', '닷', '저희', '적', '알', '쉬', '못', '꼭', '살', '제', '권', '제',\n",
        "             '분', '나', '내', '진작', '전', '뿐', '대한', '대해', '책', '좋다', '이야기','하는','통해','저자','독자','작가','위','위해','가장','중','더욱','실제','작품',\n",
        "             '가지','지금','수록','설명','내용','모든','스스로','시리즈','부','때문','그것','소개','명','속','데','이후','판','주제']\n",
        "\n",
        "# 두 불용어 리스트 합치기\n",
        "stopwords = file_stopwords + custom_stopwords\n",
        "\n",
        "# 한국어 텍스트 토큰화 및 불용어 제거 함수\n",
        "tokenizer = Okt()\n",
        "\n",
        "def tokenize_korean_text(text):\n",
        "    text = re.sub('[^가-힣a-zA-Z0-9\\s]', '', text)  # 특수문자 제거\n",
        "    morphs_tokens = tokenizer.morphs(text)  # 모든 형태소 분석 # 형태소와 명사 토큰을 합칩니다.\n",
        "    return ' '.join([token for token in morphs_tokens if token not in stopwords])\n",
        "\n",
        "# 엑셀 파일 읽기\n",
        "data = pd.read_csv('/content/book_data.csv')\n",
        "\n",
        "# 'book_introduce' 열을 전처리하고 원본 데이터프레임에 저장\n",
        "for idx in range(len(data)):\n",
        "    if pd.notnull(data.loc[idx, 'book_introduce']):\n",
        "        # NaN이 아닌 경우에만 전처리 수행\n",
        "        preprocessed_text = tokenize_korean_text(data.loc[idx, 'book_introduce'])\n",
        "        data.at[idx, 'book_introduce'] = preprocessed_text  # 전처리된 텍스트를 원본 데이터프레임에 저장\n",
        "\n",
        "    # 1000개마다 로그 찍기\n",
        "    if (idx+1) % 1000 == 0:\n",
        "        print(f\"Processed {idx+1} rows.\")\n",
        "\n",
        "# 전처리된 데이터를 피클 파일로 저장\n",
        "with open('preprocessed_book_introduce.pkl', 'wb') as file:\n",
        "    pickle.dump(data, file)  # 전체 데이터프레임을 저장"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Klue-BERT 모델 및 토크나이저 불러오기\n",
        "model = AutoModel.from_pretrained(\"klue/bert-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "# 텍스트 임베딩 함수\n",
        "def embed_text(text):\n",
        "    # 원본 텍스트를 토큰 ID로 인코딩\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512)\n",
        "\n",
        "    # 텐서로 변환\n",
        "    input_ids = torch.tensor([input_ids])\n",
        "\n",
        "    # 모델에 입력하여 마지막 은닉 상태를 얻음\n",
        "    with torch.no_grad():\n",
        "        last_hidden_states = model(input_ids)[0]\n",
        "\n",
        "    # [CLS] 토큰의 임베딩을 반환\n",
        "    return last_hidden_states[0, 0, :].numpy()\n",
        "\n",
        "# 각 도서 소개를 임베딩하고 ISBN과 함께 저장\n",
        "embedded_books_with_isbn = []\n",
        "\n",
        "for isbn, tokens in book_train_X:\n",
        "    # 토큰화된 명사들을 공백으로 구분된 문자열로 만듬\n",
        "    nouns_text = ' '.join(tokens)\n",
        "    # 해당 텍스트를 임베딩\n",
        "    embedded_text = embed_text(nouns_text)\n",
        "    # 임베딩과 ISBN을 튜플로 묶어 리스트에 추가\n",
        "    embedded_books_with_isbn.append((isbn, embedded_text))\n",
        "\n",
        "# 결과 확인\n",
        "print(embedded_books_with_isbn[:5])  # 처음 5개의 결과만 출력\n"
      ],
      "metadata": {
        "id": "pS5mjupTXmkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 도서 임베딩 벡터만 추출\n",
        "embedded_books = np.array([emb for isbn, emb in embedded_books_with_isbn])\n",
        "\n",
        "# 도서 간의 유사도 계산 함수\n",
        "def calculate_similarity(embedded_books):\n",
        "    # 모든 임베딩에 대한 코사인 유사도 계산\n",
        "    similarity_matrix = cosine_similarity(embedded_books)\n",
        "    return similarity_matrix\n",
        "\n",
        "# 도서 간 유사도 계산\n",
        "similarity_matrix = calculate_similarity(embedded_books)\n",
        "\n",
        "# 유사도 행렬 출력 (예시로 처음 5x5개의 유사도만 출력)\n",
        "print(similarity_matrix[:5, :5])\n"
      ],
      "metadata": {
        "id": "e-9zYvDGYLzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩된 벡터만을 추출합니다.\n",
        "embedded_books = np.array([emb for _, emb in embedded_books_with_isbn])\n",
        "# 만약 ISBN도 함께 저장하고 싶다면 별도의 파일로 저장할 수 있습니다.\n",
        "isbns = [isbn for isbn, _ in embedded_books_with_isbn]\n",
        "np.save('embedding_matrix.npy', isbns)\n"
      ],
      "metadata": {
        "id": "Pv_sjBEdYQBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
